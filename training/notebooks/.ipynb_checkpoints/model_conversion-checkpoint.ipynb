{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FactRadar Model Conversion to TensorFlow.js\n",
    "Convert trained models for web deployment with preprocessing pipeline.\n",
    "\n",
    "## Conversion Process:\n",
    "1. Load best trained model from model_training.ipynb\n",
    "2. Create TensorFlow/Keras wrapper for scikit-learn models\n",
    "3. Convert to TensorFlow.js format\n",
    "4. Export preprocessing parameters and vocabulary\n",
    "5. Validate converted model\n",
    "6. Prepare deployment files for React frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('üîÑ FactRadar Model Conversion Pipeline')\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'TensorFlow.js version: {tfjs.__version__}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_components():\n",
    "    \"\"\"Load the trained model and preprocessing components\"\"\"\n",
    "    \n",
    "    # Check for model metadata\n",
    "    metadata_path = \"../data/processed/models/best_model_metadata.json\"\n",
    "    if not os.path.exists(metadata_path):\n",
    "        print(\"‚ùå Model metadata not found! Please run model_training.ipynb first.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"üìÅ Loading trained model components...\")\n",
    "    print(f\"   ‚Ä¢ Model: {metadata['model_name']}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {metadata['performance_metrics']['test_accuracy']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ F1-Score: {metadata['performance_metrics']['test_f1_score']:.4f}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = joblib.load(metadata['model_path'])\n",
    "    \n",
    "    # Load vectorizer\n",
    "    vectorizer = joblib.load(metadata['vectorizer_path'])\n",
    "    \n",
    "    # Load processed dataset for training Keras model\n",
    "    data_path = \"../data/processed/fully_processed_dataset.csv\"\n",
    "    if os.path.exists(data_path):\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"   ‚Ä¢ Dataset: {len(df):,} samples\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Processed dataset not found, using minimal conversion\")\n",
    "        df = None\n",
    "    \n",
    "    return model, vectorizer, metadata, df\n",
    "\n",
    "# Load components\n",
    "sklearn_model, tfidf_vectorizer, model_metadata, dataset = load_trained_components()\n",
    "\n",
    "if sklearn_model is not None:\n",
    "    print(f\"‚úÖ Components loaded successfully!\")\n",
    "    print(f\"   ‚Ä¢ Model type: {type(sklearn_model).__name__}\")\n",
    "    print(f\"   ‚Ä¢ Feature count: {model_metadata['dataset_info']['feature_count']:,}\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed without trained model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Keras Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model_from_sklearn(sklearn_model, input_shape):\n",
    "    \"\"\"Create a Keras model that mimics the sklearn model performance\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Creating Keras model wrapper...\")\n",
    "    print(f\"   ‚Ä¢ Input shape: {input_shape}\")\n",
    "    \n",
    "    # Create neural network architecture\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Keras model created!\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "if sklearn_model is not None:\n",
    "    input_shape = model_metadata['dataset_info']['feature_count']\n",
    "    keras_model = create_keras_model_from_sklearn(sklearn_model, input_shape)\n",
    "else:\n",
    "    print(\"‚ùå No sklearn model available for conversion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Keras Model to Match Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sklearn_model is not None and dataset is not None:\n",
    "    print(f\"üîÑ Training Keras model to match sklearn performance...\")\n",
    "    \n",
    "    # Prepare features (simplified for conversion)\n",
    "    X_text = tfidf_vectorizer.transform(dataset['processed_text'].fillna(''))\n",
    "    \n",
    "    # Get numerical features\n",
    "    feature_names = model_metadata['feature_info']['feature_names']\n",
    "    X_numerical = dataset[feature_names].fillna(0)\n",
    "    \n",
    "    # Combine features\n",
    "    from scipy.sparse import hstack\n",
    "    X_combined = hstack([X_text, X_numerical.values])\n",
    "    X_dense = X_combined.toarray()  # Convert to dense for Keras\n",
    "    \n",
    "    y = dataset['label']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_dense, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Training samples: {X_train.shape[0]:,}\")\n",
    "    print(f\"   ‚Ä¢ Test samples: {X_test.shape[0]:,}\")\n",
    "    print(f\"   ‚Ä¢ Features: {X_train.shape[1]:,}\")\n",
    "    \n",
    "    # Train Keras model\n",
    "    print(f\"\\nüîÑ Training Keras model...\")\n",
    "    history = keras_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate Keras model\n",
    "    keras_loss, keras_accuracy = keras_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"\\n‚úÖ Keras model performance:\")\n",
    "    print(f\"   ‚Ä¢ Test accuracy: {keras_accuracy:.4f}\")\n",
    "    \n",
    "    # Compare with sklearn model\n",
    "    sklearn_predictions = sklearn_model.predict(X_test)\n",
    "    sklearn_accuracy = (sklearn_predictions == y_test).mean()\n",
    "    print(f\"   ‚Ä¢ Original sklearn accuracy: {sklearn_accuracy:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Performance difference: {abs(keras_accuracy - sklearn_accuracy):.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot train Keras model without data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert to TensorFlow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'keras_model' in locals():\n",
    "    # Convert to TensorFlow.js\n",
    "    output_path = '../../models/tfjs_model'\n",
    "    \n",
    "    print(f\"üîÑ Converting to TensorFlow.js...\")\n",
    "    print(f\"   ‚Ä¢ Output path: {output_path}\")\n",
    "    \n",
    "    # Create models directory\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Convert model\n",
    "    tfjs.converters.save_keras_model(\n",
    "        keras_model, \n",
    "        output_path,\n",
    "        quantization_bytes=2  # Quantize to reduce model size\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model conversion completed!\")\n",
    "    \n",
    "    # Check output files\n",
    "    if os.path.exists(output_path):\n",
    "        files = os.listdir(output_path)\n",
    "        print(f\"\\nüìÅ Generated files:\")\n",
    "        for file in files:\n",
    "            file_path = os.path.join(output_path, file)\n",
    "            size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "            print(f\"   ‚Ä¢ {file}: {size:.2f} KB\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No Keras model available for conversion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Preprocessing Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tfidf_vectorizer is not None:\n",
    "    print(f\"üîÑ Exporting preprocessing components...\")\n",
    "    \n",
    "    # Export TF-IDF vocabulary\n",
    "    vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "    vocab_dict = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    vocab_path = '../../models/vocabulary.json'\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        json.dump(vocab_dict, f)\n",
    "    \n",
    "    print(f\"‚úÖ Vocabulary exported: {vocab_path}\")\n",
    "    print(f\"   ‚Ä¢ Vocabulary size: {len(vocab_dict):,}\")\n",
    "    \n",
    "    # Export preprocessing parameters\n",
    "    preprocessing_config = {\n",
    "        'model_info': {\n",
    "            'name': model_metadata['model_name'],\n",
    "            'type': model_metadata['model_type'],\n",
    "            'accuracy': model_metadata['performance_metrics']['test_accuracy'],\n",
    "            'f1_score': model_metadata['performance_metrics']['test_f1_score']\n",
    "        },\n",
    "        'tfidf_params': {\n",
    "            'max_features': len(vocab_dict),\n",
    "            'ngram_range': [1, 2],\n",
    "            'stop_words': 'english',\n",
    "            'lowercase': True,\n",
    "            'sublinear_tf': True\n",
    "        },\n",
    "        'feature_engineering': {\n",
    "            'numerical_features': model_metadata['feature_info']['feature_names'],\n",
    "            'total_features': model_metadata['dataset_info']['feature_count']\n",
    "        },\n",
    "        'text_processing': {\n",
    "            'steps': [\n",
    "                'HTML tag removal',\n",
    "                'URL removal', \n",
    "                'Punctuation normalization',\n",
    "                'Tokenization',\n",
    "                'Stopword removal',\n",
    "                'Stemming',\n",
    "                'TF-IDF vectorization'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = '../../models/preprocessing_config.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(preprocessing_config, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing config exported: {config_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No vectorizer available for export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation and summary\n",
    "print(f\"üéØ CONVERSION VALIDATION & SUMMARY\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "if 'keras_model' in locals() and tfidf_vectorizer is not None:\n",
    "    print(f\"‚úÖ Conversion completed successfully!\")\n",
    "    print(f\"\\nüìä Model Information:\")\n",
    "    print(f\"   ‚Ä¢ Original model: {model_metadata['model_name']}\")\n",
    "    print(f\"   ‚Ä¢ Original accuracy: {model_metadata['performance_metrics']['test_accuracy']:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Keras accuracy: {keras_accuracy:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Feature count: {model_metadata['dataset_info']['feature_count']:,}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Deployment Files:\")\n",
    "    print(f\"   ‚Ä¢ TensorFlow.js model: models/tfjs_model/\")\n",
    "    print(f\"   ‚Ä¢ Vocabulary: models/vocabulary.json\")\n",
    "    print(f\"   ‚Ä¢ Config: models/preprocessing_config.json\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready for Frontend Integration!\")\n",
    "    print(f\"\\nüìã Next Steps:\")\n",
    "    print(f\"   1. Integrate TensorFlow.js model with React frontend\")\n",
    "    print(f\"   2. Implement client-side preprocessing\")\n",
    "    print(f\"   3. Add real-time prediction interface\")\n",
    "    print(f\"   4. Test end-to-end functionality\")\n",
    "    print(f\"   5. Deploy to production\")\n",
    "    \n",
    "    print(f\"\\nüí° Integration Notes:\")\n",
    "    print(f\"   ‚Ä¢ Model expects {model_metadata['dataset_info']['feature_count']:,} features\")\n",
    "    print(f\"   ‚Ä¢ Text preprocessing must match training pipeline\")\n",
    "    print(f\"   ‚Ä¢ Use vocabulary.json for consistent tokenization\")\n",
    "    print(f\"   ‚Ä¢ Expected inference time: < 100ms\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Conversion incomplete!\")\n",
    "    print(f\"   ‚Ä¢ Check that model_training.ipynb has been run successfully\")\n",
    "    print(f\"   ‚Ä¢ Ensure all required files are present\")\n",
    "\n",
    "print(f\"\\nüéâ Model conversion pipeline completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
