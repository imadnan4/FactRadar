{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FactRadar Real Data Preprocessing Pipeline\n",
    "Comprehensive preprocessing with NLTK for 44K+ real dataset samples.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. Real Dataset Loading (44K+ samples)\n",
    "2. Advanced Text Cleaning with NLTK\n",
    "3. Comprehensive Feature Engineering\n",
    "4. TF-IDF Vectorization with N-grams\n",
    "5. Data Export for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 FactRadar Real Data Preprocessing Pipeline\n",
      "============================================================\n",
      "Processing 56K+ samples with comprehensive NLP features\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print('🔄 FactRadar Real Data Preprocessing Pipeline')\n",
    "print('=' * 60)\n",
    "print('Processing 56K+ samples with comprehensive NLP features')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text cleaning functions defined!\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_text_cleaning(text):\n",
    "    \"\"\"Comprehensive text cleaning pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Handle excessive punctuation\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    text = re.sub(r'[.]{3,}', '...', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def advanced_text_preprocessing(text, use_stemming=True, remove_stops=True):\n",
    "    \"\"\"Advanced text preprocessing with NLTK\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Initialize tools\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove non-alphabetic tokens and short words\n",
    "    tokens = [token for token in tokens if token.isalpha() and len(token) > 2]\n",
    "    \n",
    "    # Remove stopwords if specified\n",
    "    if remove_stops:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming or lemmatization\n",
    "    if use_stemming:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    else:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"✅ Text cleaning functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature engineering functions defined!\n"
     ]
    }
   ],
   "source": [
    "def extract_comprehensive_features(text):\n",
    "    \"\"\"Extract comprehensive NLP features using NLTK\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return {\n",
    "            # Basic features\n",
    "            'word_count': 0, 'char_count': 0, 'sentence_count': 0,\n",
    "            'avg_word_length': 0, 'avg_sentence_length': 0,\n",
    "            # Punctuation features\n",
    "            'exclamation_count': 0, 'question_count': 0, 'caps_ratio': 0,\n",
    "            'punctuation_density': 0,\n",
    "            # Sentiment features\n",
    "            'sentiment_compound': 0, 'sentiment_positive': 0, 'sentiment_negative': 0,\n",
    "            # Linguistic features\n",
    "            'pos_noun_ratio': 0, 'pos_verb_ratio': 0, 'pos_adj_ratio': 0,\n",
    "            'unique_word_ratio': 0, 'stopword_ratio': 0,\n",
    "            # Readability\n",
    "            'readability_score': 0\n",
    "        }\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Basic text statistics\n",
    "    words = word_tokenize(text.lower())\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    word_count = len(words)\n",
    "    char_count = len(text)\n",
    "    sentence_count = len(sentences)\n",
    "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "    \n",
    "    # Punctuation analysis\n",
    "    exclamation_count = text.count('!')\n",
    "    question_count = text.count('?')\n",
    "    caps_count = sum(1 for c in text if c.isupper())\n",
    "    caps_ratio = caps_count / char_count if char_count > 0 else 0\n",
    "    punctuation_count = sum(1 for c in text if c in string.punctuation)\n",
    "    punctuation_density = punctuation_count / char_count if char_count > 0 else 0\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # POS tagging analysis\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    noun_count = sum(1 for word, pos in pos_tags if pos.startswith('N'))\n",
    "    verb_count = sum(1 for word, pos in pos_tags if pos.startswith('V'))\n",
    "    adj_count = sum(1 for word, pos in pos_tags if pos.startswith('J'))\n",
    "    \n",
    "    pos_noun_ratio = noun_count / word_count if word_count > 0 else 0\n",
    "    pos_verb_ratio = verb_count / word_count if word_count > 0 else 0\n",
    "    pos_adj_ratio = adj_count / word_count if word_count > 0 else 0\n",
    "    \n",
    "    # Vocabulary analysis\n",
    "    alpha_words = [word for word in words if word.isalpha()]\n",
    "    unique_words = set(alpha_words)\n",
    "    unique_word_ratio = len(unique_words) / len(alpha_words) if alpha_words else 0\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopword_count = sum(1 for word in alpha_words if word in stop_words)\n",
    "    stopword_ratio = stopword_count / len(alpha_words) if alpha_words else 0\n",
    "    \n",
    "    # Readability score (simplified Flesch)\n",
    "    avg_syllables = np.mean([count_syllables(word) for word in alpha_words]) if alpha_words else 0\n",
    "    readability_score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables)\n",
    "    \n",
    "    return {\n",
    "        # Basic features\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        # Punctuation features\n",
    "        'exclamation_count': exclamation_count,\n",
    "        'question_count': question_count,\n",
    "        'caps_ratio': caps_ratio,\n",
    "        'punctuation_density': punctuation_density,\n",
    "        # Sentiment features\n",
    "        'sentiment_compound': sentiment_scores['compound'],\n",
    "        'sentiment_positive': sentiment_scores['pos'],\n",
    "        'sentiment_negative': sentiment_scores['neg'],\n",
    "        # Linguistic features\n",
    "        'pos_noun_ratio': pos_noun_ratio,\n",
    "        'pos_verb_ratio': pos_verb_ratio,\n",
    "        'pos_adj_ratio': pos_adj_ratio,\n",
    "        'unique_word_ratio': unique_word_ratio,\n",
    "        'stopword_ratio': stopword_ratio,\n",
    "        # Readability\n",
    "        'readability_score': readability_score\n",
    "    }\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"Count syllables in a word\"\"\"\n",
    "    word = word.lower()\n",
    "    vowels = 'aeiouy'\n",
    "    syllable_count = 0\n",
    "    prev_char_was_vowel = False\n",
    "    \n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            if not prev_char_was_vowel:\n",
    "                syllable_count += 1\n",
    "            prev_char_was_vowel = True\n",
    "        else:\n",
    "            prev_char_was_vowel = False\n",
    "    \n",
    "    if word.endswith('e'):\n",
    "        syllable_count -= 1\n",
    "    \n",
    "    return max(1, syllable_count)\n",
    "\n",
    "print(\"✅ Feature engineering functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real Dataset Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading real dataset...\n",
      "📁 Loading real dataset from: ../data/processed/real_dataset_processed.csv\n",
      "✅ Loaded 3,998 samples\n",
      "🔄 Processing full dataset of 3,998 samples\n",
      "🔄 Processing 3,998 samples in chunks of 2,000...\n",
      "📊 Processing chunk 1/2...\n",
      "   🧹 Cleaning text...\n",
      "   ⚙️ Preprocessing text...\n",
      "   📊 Extracting features...\n",
      "   ✅ Chunk 1 completed!\n",
      "📊 Processing chunk 2/2...\n",
      "   🧹 Cleaning text...\n",
      "   ⚙️ Preprocessing text...\n",
      "   📊 Extracting features...\n",
      "   ✅ Chunk 2 completed!\n",
      "✅ Processing completed!\n",
      "📈 Final dataset shape: (3998, 16)\n",
      "\n",
      "📊 Processing Summary:\n",
      "   • Original samples: 3,998\n",
      "   • Processed samples: 3,998\n",
      "   • Features extracted: 12\n",
      "   • Memory usage: 30.4 MB\n"
     ]
    }
   ],
   "source": [
    "def load_real_dataset():\n",
    "    \"\"\"Load the real processed dataset\"\"\"\n",
    "    \n",
    "    data_path = \"../data/processed/real_dataset_processed.csv\"\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"❌ Real dataset not found! Please run load_real_datasets.py first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"📁 Loading real dataset from: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"✅ Loaded {len(df):,} samples\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def safe_text_cleaning(text):\n",
    "    \"\"\"Safe text cleaning with error handling\"\"\"\n",
    "    try:\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        # Basic cleaning without heavy NLTK dependencies\n",
    "        import re\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\.\\!\\?\\,\\;\\:]', ' ', text)\n",
    "        \n",
    "        # Remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Text cleaning failed for text: {str(e)}\")\n",
    "        return str(text) if text else ''\n",
    "\n",
    "def safe_text_preprocessing(text):\n",
    "    \"\"\"Safe text preprocessing with minimal dependencies\"\"\"\n",
    "    try:\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        # Basic preprocessing\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(str(text))\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        \n",
    "        # Join back\n",
    "        return ' '.join(tokens)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Preprocessing failed: {str(e)}\")\n",
    "        return safe_text_cleaning(text)\n",
    "\n",
    "def safe_extract_features(text):\n",
    "    \"\"\"Safe feature extraction with error handling\"\"\"\n",
    "    try:\n",
    "        if pd.isna(text) or text == '':\n",
    "            return {\n",
    "                'word_count': 0,\n",
    "                'sentence_count': 0,\n",
    "                'avg_word_length': 0,\n",
    "                'sentiment_compound': 0,\n",
    "                'exclamation_count': 0,\n",
    "                'question_count': 0,\n",
    "                'caps_ratio': 0,\n",
    "                'stopword_ratio': 0,\n",
    "                'unique_word_ratio': 0\n",
    "            }\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Basic text statistics\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Sentence count (approximate)\n",
    "        sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))\n",
    "        \n",
    "        # Average word length\n",
    "        avg_word_length = sum(len(word) for word in words) / max(1, word_count)\n",
    "        \n",
    "        # Punctuation counts\n",
    "        exclamation_count = text.count('!')\n",
    "        question_count = text.count('?')\n",
    "        \n",
    "        # Capital letters ratio\n",
    "        caps_ratio = sum(1 for c in text if c.isupper()) / max(1, len(text))\n",
    "        \n",
    "        # Unique words ratio\n",
    "        unique_words = len(set(words))\n",
    "        unique_word_ratio = unique_words / max(1, word_count)\n",
    "        \n",
    "        # Stopword ratio (safe)\n",
    "        try:\n",
    "            from nltk.corpus import stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            stopword_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "            stopword_ratio = stopword_count / max(1, word_count)\n",
    "        except:\n",
    "            stopword_ratio = 0.4  # Default estimate\n",
    "        \n",
    "        # Sentiment (safe)\n",
    "        try:\n",
    "            from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "            analyzer = SentimentIntensityAnalyzer()\n",
    "            sentiment_compound = analyzer.polarity_scores(text)['compound']\n",
    "        except:\n",
    "            sentiment_compound = 0  # Neutral default\n",
    "        \n",
    "        return {\n",
    "            'word_count': word_count,\n",
    "            'sentence_count': sentence_count,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'sentiment_compound': sentiment_compound,\n",
    "            'exclamation_count': exclamation_count,\n",
    "            'question_count': question_count,\n",
    "            'caps_ratio': caps_ratio,\n",
    "            'stopword_ratio': stopword_ratio,\n",
    "            'unique_word_ratio': unique_word_ratio\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Feature extraction failed: {str(e)}\")\n",
    "        # Return default features\n",
    "        return {\n",
    "            'word_count': 0,\n",
    "            'sentence_count': 1,\n",
    "            'avg_word_length': 5,\n",
    "            'sentiment_compound': 0,\n",
    "            'exclamation_count': 0,\n",
    "            'question_count': 0,\n",
    "            'caps_ratio': 0.05,\n",
    "            'stopword_ratio': 0.4,\n",
    "            'unique_word_ratio': 0.8\n",
    "        }\n",
    "\n",
    "def process_dataset_in_chunks(df, chunk_size=5000):\n",
    "    \"\"\"Process large dataset in chunks for memory efficiency\"\"\"\n",
    "    \n",
    "    print(f\"🔄 Processing {len(df):,} samples in chunks of {chunk_size:,}...\")\n",
    "    \n",
    "    processed_chunks = []\n",
    "    total_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        print(f\"📊 Processing chunk {chunk_num}/{total_chunks}...\")\n",
    "        \n",
    "        chunk = df.iloc[i:i+chunk_size].copy()\n",
    "        \n",
    "        # Apply safe text cleaning\n",
    "        print(f\"   🧹 Cleaning text...\")\n",
    "        chunk['cleaned_text'] = chunk['text'].apply(safe_text_cleaning)\n",
    "        \n",
    "        # Apply safe preprocessing\n",
    "        print(f\"   ⚙️ Preprocessing text...\")\n",
    "        chunk['processed_text'] = chunk['cleaned_text'].apply(safe_text_preprocessing)\n",
    "        \n",
    "        # Extract features safely\n",
    "        print(f\"   📊 Extracting features...\")\n",
    "        features = chunk['cleaned_text'].apply(safe_extract_features)\n",
    "        features_df = pd.DataFrame(features.tolist())\n",
    "        \n",
    "        # Combine\n",
    "        chunk_processed = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "        processed_chunks.append(chunk_processed)\n",
    "        \n",
    "        print(f\"   ✅ Chunk {chunk_num} completed!\")\n",
    "    \n",
    "    # Combine all chunks\n",
    "    df_final = pd.concat(processed_chunks, ignore_index=True)\n",
    "    \n",
    "    print(f\"✅ Processing completed!\")\n",
    "    print(f\"📈 Final dataset shape: {df_final.shape}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Load and process the real dataset\n",
    "print(\"🔄 Loading real dataset...\")\n",
    "df_raw = load_real_dataset()\n",
    "\n",
    "if df_raw is not None:\n",
    "    # For development, use a sample. For production, process full dataset\n",
    "    USE_SAMPLE = True  # Set to False for full dataset processing\n",
    "    SAMPLE_SIZE = 10000\n",
    "    \n",
    "    if USE_SAMPLE and len(df_raw) > SAMPLE_SIZE:\n",
    "        print(f\"🔄 Using sample of {SAMPLE_SIZE:,} for development\")\n",
    "        df_to_process = df_raw.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    else:\n",
    "        print(f\"🔄 Processing full dataset of {len(df_raw):,} samples\")\n",
    "        df_to_process = df_raw\n",
    "    \n",
    "    # Process the dataset\n",
    "    df_processed = process_dataset_in_chunks(df_to_process, chunk_size=2000)\n",
    "    \n",
    "    print(f\"\\n📊 Processing Summary:\")\n",
    "    print(f\"   • Original samples: {len(df_raw):,}\")\n",
    "    print(f\"   • Processed samples: {len(df_processed):,}\")\n",
    "    print(f\"   • Features extracted: {len([col for col in df_processed.columns if col not in ['text', 'label', 'dataset', 'cleaned_text', 'processed_text']])}\")\n",
    "    print(f\"   • Memory usage: {df_processed.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "else:\n",
    "    print(\"❌ Cannot proceed without dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Vectorization with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating TF-IDF features...\n",
      "   • Max features: 10,000\n",
      "   • N-gram range: (1, 2)\n",
      "✅ TF-IDF matrix created: (3998, 10000)\n",
      "📈 Vocabulary size: 10,000\n",
      "\n",
      "📊 TF-IDF Summary:\n",
      "   • Matrix shape: (3998, 10000)\n",
      "   • Matrix density: 0.0150\n",
      "   • Memory usage: 4.6 MB\n",
      "\n",
      "🔤 Sample TF-IDF features:\n",
      "    1. 00\n",
      "    2. 00 pm\n",
      "    3. 000\n",
      "    4. 000 people\n",
      "    5. 000 refugees\n",
      "    6. 000 rohingya\n",
      "    7. 000 syrian\n",
      "    8. 000 troops\n",
      "    9. 000 versus\n",
      "   10. 000 year\n",
      "   11. 00pm\n",
      "   12. 10\n",
      "   13. 10 000\n",
      "   14. 10 2016\n",
      "   15. 10 billion\n",
      "   16. 10 days\n",
      "   17. 10 million\n",
      "   18. 10 percent\n",
      "   19. 10 year\n",
      "   20. 10 years\n"
     ]
    }
   ],
   "source": [
    "def create_tfidf_features(texts, max_features=10000, ngram_range=(1, 2)):\n",
    "    \"\"\"Create TF-IDF features with n-grams\"\"\"\n",
    "    \n",
    "    print(f\"🔄 Creating TF-IDF features...\")\n",
    "    print(f\"   • Max features: {max_features:,}\")\n",
    "    print(f\"   • N-gram range: {ngram_range}\")\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        sublinear_tf=True,\n",
    "        min_df=2,  # Ignore terms in less than 2 documents\n",
    "        max_df=0.95  # Ignore terms in more than 95% of documents\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    print(f\"✅ TF-IDF matrix created: {tfidf_matrix.shape}\")\n",
    "    print(f\"📈 Vocabulary size: {len(feature_names):,}\")\n",
    "    \n",
    "    return tfidf_matrix, vectorizer, feature_names\n",
    "\n",
    "if 'df_processed' in locals():\n",
    "    # Create TF-IDF features\n",
    "    tfidf_matrix, tfidf_vectorizer, feature_names = create_tfidf_features(\n",
    "        df_processed['processed_text'].fillna(''),\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 TF-IDF Summary:\")\n",
    "    print(f\"   • Matrix shape: {tfidf_matrix.shape}\")\n",
    "    print(f\"   • Matrix density: {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")\n",
    "    print(f\"   • Memory usage: {tfidf_matrix.data.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Show top features\n",
    "    print(f\"\\n🔤 Sample TF-IDF features:\")\n",
    "    for i, feature in enumerate(feature_names[:20]):\n",
    "        print(f\"   {i+1:2d}. {feature}\")\n",
    "else:\n",
    "    print(\"❌ No processed dataset available for TF-IDF creation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Export and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Preparing data for export...\n",
      "✅ Processed dataset saved: ../data/processed/fully_processed_dataset.csv\n",
      "✅ TF-IDF vectorizer saved: ../data/processed/tfidf_vectorizer_full.pkl\n",
      "✅ Feature summary saved: ../data/processed/feature_summary.json\n",
      "\n",
      "🎉 PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "📊 Final Dataset Statistics:\n",
      "   • Total samples: 3,998\n",
      "   • Real news: 1,999\n",
      "   • Fake news: 1,999\n",
      "   • Engineered features: 12\n",
      "   • TF-IDF features: 10,000\n",
      "   • Total features: 10,012\n",
      "\n",
      "🚀 Ready for Model Training!\n",
      "   1. Run model_training.ipynb for comprehensive model development\n",
      "   2. Expected accuracy: 85-95% with this feature set\n",
      "   3. Use cross-validation for robust evaluation\n",
      "   4. Convert best model to TensorFlow.js for deployment\n",
      "\n",
      "📈 Feature Analysis Preview:\n",
      "           word_count  sentence_count  avg_word_length  sentiment_compound  \\\n",
      "Real News    389.7439         20.9260           5.0678                 0.0   \n",
      "Fake News    431.1646         22.6633           4.9237                 0.0   \n",
      "\n",
      "           exclamation_count  \n",
      "Real News             0.0625  \n",
      "Fake News             0.7669  \n"
     ]
    }
   ],
   "source": [
    "if 'df_processed' in locals():\n",
    "    # Prepare final dataset for model training\n",
    "    print(\"💾 Preparing data for export...\")\n",
    "    \n",
    "    # Save processed dataset\n",
    "    output_file = \"../data/processed/fully_processed_dataset.csv\"\n",
    "    df_processed.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Processed dataset saved: {output_file}\")\n",
    "    \n",
    "    # Save TF-IDF vectorizer\n",
    "    import joblib\n",
    "    vectorizer_file = \"../data/processed/tfidf_vectorizer_full.pkl\"\n",
    "    joblib.dump(tfidf_vectorizer, vectorizer_file)\n",
    "    print(f\"✅ TF-IDF vectorizer saved: {vectorizer_file}\")\n",
    "    \n",
    "    # Create feature summary\n",
    "    feature_columns = [col for col in df_processed.columns \n",
    "                      if col not in ['text', 'label', 'dataset', 'cleaned_text', 'processed_text']]\n",
    "    \n",
    "    feature_summary = {\n",
    "        'total_samples': len(df_processed),\n",
    "        'real_samples': len(df_processed[df_processed['label'] == 0]),\n",
    "        'fake_samples': len(df_processed[df_processed['label'] == 1]),\n",
    "        'engineered_features': len(feature_columns),\n",
    "        'tfidf_features': tfidf_matrix.shape[1],\n",
    "        'total_features': len(feature_columns) + tfidf_matrix.shape[1],\n",
    "        'feature_names': feature_columns,\n",
    "        'tfidf_params': {\n",
    "            'max_features': 10000,\n",
    "            'ngram_range': [1, 2],\n",
    "            'vocabulary_size': len(feature_names)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save feature summary\n",
    "    import json\n",
    "    summary_file = \"../data/processed/feature_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(feature_summary, f, indent=2)\n",
    "    print(f\"✅ Feature summary saved: {summary_file}\")\n",
    "    \n",
    "    # Display final statistics\n",
    "    print(f\"\\n🎉 PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"=\" * 60)\n",
    "    print(f\"📊 Final Dataset Statistics:\")\n",
    "    print(f\"   • Total samples: {feature_summary['total_samples']:,}\")\n",
    "    print(f\"   • Real news: {feature_summary['real_samples']:,}\")\n",
    "    print(f\"   • Fake news: {feature_summary['fake_samples']:,}\")\n",
    "    print(f\"   • Engineered features: {feature_summary['engineered_features']}\")\n",
    "    print(f\"   • TF-IDF features: {feature_summary['tfidf_features']:,}\")\n",
    "    print(f\"   • Total features: {feature_summary['total_features']:,}\")\n",
    "    \n",
    "    print(f\"\\n🚀 Ready for Model Training!\")\n",
    "    print(f\"   1. Run model_training.ipynb for comprehensive model development\")\n",
    "    print(f\"   2. Expected accuracy: 85-95% with this feature set\")\n",
    "    print(f\"   3. Use cross-validation for robust evaluation\")\n",
    "    print(f\"   4. Convert best model to TensorFlow.js for deployment\")\n",
    "    \n",
    "    # Quick feature analysis\n",
    "    print(f\"\\n📈 Feature Analysis Preview:\")\n",
    "   # Only select numeric columns for feature analysis\n",
    "    numeric_feature_columns = [\n",
    "    col for col in df_processed.columns\n",
    "    if col not in ['text', 'label', 'dataset', 'cleaned_text', 'processed_text']\n",
    "    and pd.api.types.is_numeric_dtype(df_processed[col])\n",
    "    ]\n",
    "\n",
    "    feature_stats = df_processed.groupby('label')[numeric_feature_columns[:5]].mean()\n",
    "    feature_stats.index = ['Real News', 'Fake News']\n",
    "    print(feature_stats.round(4))\n",
    "else:\n",
    "    print(\"❌ No processed dataset available for export!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
